{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799339fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy --upgrade\n",
    "# !pip install scipy --upgrade\n",
    "# !pip install --user --force-reinstall numpy==1.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad668c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451ffc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to generate dummy data\n",
    "def generate_dummy_data(num_batches, events_per_batch, pulses_per_event, sensors):\n",
    "    batches_meta = []\n",
    "    batches_data = []\n",
    "    \n",
    "    for batch_id in range(num_batches):\n",
    "        for event_id in range(events_per_batch):\n",
    "            first_pulse_index = event_id * pulses_per_event\n",
    "            last_pulse_index = (event_id + 1) * pulses_per_event - 1\n",
    "            azimuth = np.random.uniform(0, 2 * np.pi)\n",
    "            zenith = np.random.uniform(0, np.pi)\n",
    "            \n",
    "            batches_meta.append([batch_id, event_id, first_pulse_index, last_pulse_index, azimuth, zenith])\n",
    "            \n",
    "            for pulse_id in range(pulses_per_event):\n",
    "                time = np.random.randint(0, 10000)\n",
    "                sensor_id = np.random.randint(0, sensors)\n",
    "                charge = np.random.normal(1, 0.1)\n",
    "                auxiliary = np.random.choice([True, False])\n",
    "                \n",
    "                batches_data.append([event_id, time, sensor_id, charge, auxiliary])\n",
    "    \n",
    "    meta_df = pd.DataFrame(batches_meta, columns=['batch_id', 'event_id', 'first_pulse_index', 'last_pulse_index', 'azimuth', 'zenith'])\n",
    "    data_df = pd.DataFrame(batches_data, columns=['event_id', 'time', 'sensor_id', 'charge', 'auxiliary'])\n",
    "    \n",
    "    return meta_df, data_df\n",
    "\n",
    "# Parameters for dummy data generation\n",
    "num_batches = 10\n",
    "events_per_batch = 100\n",
    "pulses_per_event = 50\n",
    "sensors = 5160\n",
    "\n",
    "train_meta, train_data = generate_dummy_data(num_batches, events_per_batch, pulses_per_event, sensors)\n",
    "train_meta.to_parquet('train_meta.parquet')\n",
    "train_data.to_parquet('train_data.parquet')\n",
    "\n",
    "# Load the data\n",
    "train_meta = pd.read_parquet('train_meta.parquet')\n",
    "train_data = pd.read_parquet('train_data.parquet')\n",
    "\n",
    "# Normalize the sensor IDs and times\n",
    "train_data['sensor_id'] = train_data['sensor_id'] / train_data['sensor_id'].max()\n",
    "train_data['time'] = train_data['time'] / train_data['time'].max()\n",
    "\n",
    "# Combine the data into one dataset for each event\n",
    "def combine_event_data(event_id, train_data, max_pulses):\n",
    "    event_data = train_data[train_data['event_id'] == event_id]\n",
    "    if len(event_data) < max_pulses:\n",
    "        padding = pd.DataFrame(np.zeros((max_pulses - len(event_data), len(event_data.columns))), columns=event_data.columns)\n",
    "        event_data = pd.concat([event_data, padding])\n",
    "    return np.array(event_data[['time', 'sensor_id', 'charge', 'auxiliary']].values, dtype=np.float32)\n",
    "\n",
    "max_pulses = pulses_per_event\n",
    "train_meta['event_data'] = train_meta['event_id'].apply(lambda x: combine_event_data(x, train_data, max_pulses))\n",
    "\n",
    "class NeutrinoModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeutrinoModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=4, hidden_size=64, num_layers=2, batch_first=True)\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)  # Predicting azimuth and zenith\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_lstm, _ = self.lstm(x)\n",
    "        h_lstm = h_lstm[:, -1, :]  # Take the last output of the LSTM\n",
    "        x = torch.relu(self.fc1(h_lstm))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = NeutrinoModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Prepare the data for training\n",
    "X = np.stack(train_meta['event_data'].values)\n",
    "y = train_meta[['azimuth', 'zenith']].values\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    permutation = torch.randperm(X_train.size()[0])\n",
    "\n",
    "    for i in range(0, X_train.size()[0], batch_size):\n",
    "        indices = permutation[i:i + batch_size]\n",
    "        batch_x, batch_y = X_train[indices], y_train[indices]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val)\n",
    "        val_loss = criterion(val_outputs, y_val)\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'neutrino_model.pth')\n",
    "\n",
    "# Load the test data\n",
    "test_meta = pd.read_parquet('test_meta.parquet')\n",
    "test_data = pd.read_parquet('test_data.parquet')\n",
    "\n",
    "# Preprocess the test data similarly\n",
    "test_data['sensor_id'] = test_data['sensor_id'] / test_data['sensor_id'].max()\n",
    "test_data['time'] = test_data['time'] / test_data['time'].max()\n",
    "test_meta['event_data'] = test_meta['event_id'].apply(lambda x: combine_event_data(x, test_data, max_pulses))\n",
    "\n",
    "# Prepare the test data\n",
    "X_test = np.stack(test_meta['event_data'].values)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# Load the model\n",
    "model.load_state_dict(torch.load('neutrino_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test)\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "submission = pd.DataFrame({'event_id': test_meta['event_id'], 'azimuth': predictions[:, 0].numpy(), 'zenith': predictions[:, 1].numpy()})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "\n",
    "# epoch loss and epoch validation loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1eb350",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
